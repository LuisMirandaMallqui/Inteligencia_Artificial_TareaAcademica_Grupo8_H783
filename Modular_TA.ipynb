{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisMirandaMallqui/Inteligencia_Artificial_TareaAcademica_Grupo8_H783/blob/main/Modular_TA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Claro que sí\\! He revisado tu notebook (`Grupo8-TA-Final_v2.pdf`) y veo que has hecho un gran trabajo en la carga, renombrado y limpieza inicial. Estás justo en el punto más importante: **cómo estructurar el preprocesamiento y el modelado**.\n",
        "\n",
        "Tu profesor tiene toda la razón. El \"PELIGRO\" que anotas en tu PDF es real: aplicar normalización y encoding a todo el dataset *antes* de dividirlo es un error común llamado **\"fuga de datos\" (data leakage)**.\n",
        "\n",
        "La solución es el flujo que te recomendé. He reestructurado tu notebook en un script modular y lógico que sigue esta metodología. Este código está diseñado para que lo copies y pegues, reemplazando tu sección de preprocesamiento y modelado.\n",
        "\n",
        "-----\n",
        "\n",
        "## **Flujo de Trabajo Corregido y Modular (Notebook Completo)**\n",
        "\n",
        "Este código toma tu limpieza, la organiza y la integra con los `Pipelines` y el `ColumnTransformer` que hemos discutido, respetando las indicaciones de tu profesor."
      ],
      "metadata": {
        "id": "GUzubRw3xHRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. LIBRERÍAS\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Modelos\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Métricas\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.model_selection import train_test_split # Lo usaremos, pero cronológicamente\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Configuración\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 7)\n",
        "print(\"Librerías importadas.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CARGA Y RENOMBRADO DE DATOS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 2. Cargando y Renombrando Datos ---\")\n",
        "try:\n",
        "    # Cargamos especificando dtypes de columnas problemáticas como 'str'\n",
        "    df_nacimientos = pd.read_csv(\n",
        "        'CNV_MINSA_4782338_CORTE_310825.csv',\n",
        "        delimiter=';',\n",
        "        on_bad_lines='skip',\n",
        "        dtype={\n",
        "            'Num_embar_madre': 'str',\n",
        "            'Hijos_vivo_madre': 'str',\n",
        "            'Hijos_fallec_madre': 'str',\n",
        "            'nacmuer_abort_madre': 'str'\n",
        "        }\n",
        "    )\n",
        "    df_ubigeos = pd.read_csv('Lista_Ubigeos_INEI.csv', delimiter=';')\n",
        "    print(\"Archivos cargados exitosamente.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Asegúrate de que los archivos CSV estén en la ubicación correcta.\")\n",
        "    # (Detener la ejecución o manejar el error)\n",
        "\n",
        "# Diccionario de renombrado (tomado de tu notebook)\n",
        "df_nacimientos.rename(columns={\n",
        "    'FecNac_Año': 'Año_Nacimiento',\n",
        "    'FecNac_Mes': 'Mes_Nacimiento',\n",
        "    'PESO_NACIDO': 'Peso_Nacido',\n",
        "    'TALLA_NACIDO': 'Talla_Nacido',\n",
        "    'DUR_EMB_PARTO': 'Duracion_Embarazo',\n",
        "    'Condicion_Parto': 'Condicion_Parto',\n",
        "    'sexo_nacido': 'Sexo_Nacido',\n",
        "    'Tipo_Parto': 'Tipo_Parto',\n",
        "    'Edad_Madre': 'Edad_Madre',\n",
        "    'Estado_Civil': 'Estado_Civil_Madre',\n",
        "    'Nivel_Intrucción_Madre': 'Nivel_Instruccion_Madre',\n",
        "    'DESC_OCUPACION': 'Ocupacion_Madre',\n",
        "    'Num_embar_madre': 'Numero_Embarazos_Madre',\n",
        "    'Hijos_vivo_madre': 'Hijos_Vivos_Madre',\n",
        "    'Hijos_fallec_madre': 'Hijos_Fallecidos_Madre',\n",
        "    'nacmuer_abort_madre': 'Abortos_Madre',\n",
        "    'Pais_Madre': 'Pais_Madre',\n",
        "    'IdUbigeoInei': 'Codigo_Ubigeo',\n",
        "    'Ipress': 'Ipress_Hospital',\n",
        "    'Lugar_Nacido': 'Lugar_Nacimiento',\n",
        "    'Atiende_Parto': 'Atiende_Parto',\n",
        "    'Financiador_Parto': 'Financiador_Parto'\n",
        "}, inplace=True)\n",
        "print(\"Columnas renombradas.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. FUNCIÓN DE LIMPIEZA DE COLUMNAS NUMÉRICAS\n",
        "# ==============================================================================\n",
        "def limpiar_columna_conteo(columna, mapeo_extra={}):\n",
        "    \"\"\"\n",
        "    Limpia una columna de conteo (como 'Hijos') que viene como texto.\n",
        "    Convierte 'NINGUNO', 'IGNORADO', '-1' y mapeos extra a valores numéricos.\n",
        "    \"\"\"\n",
        "    # Mapeo base\n",
        "    mapeo_base = {\n",
        "        'NINGUNO': 0,\n",
        "        'IGNORADO': 0, # Decidimos tratar 'IGNORADO' como 0\n",
        "        '-1': 0        # Tratar -1 como 0\n",
        "    }\n",
        "\n",
        "    # Combinar con mapeos específicos (ej. '11 a más': 11)\n",
        "    mapeo_total = {**mapeo_base, **mapeo_extra}\n",
        "\n",
        "    columna_limpia = columna.replace(mapeo_total)\n",
        "\n",
        "    # Convertir a numérico, todo lo que no sea número se vuelve NaN\n",
        "    columna_num = pd.to_numeric(columna_limpia, errors='coerce')\n",
        "\n",
        "    # Rellenar NaNs (los 'coerce' y los originales) con 0\n",
        "    columna_final = columna_num.fillna(0)\n",
        "\n",
        "    return columna_final.astype(int)\n",
        "\n",
        "print(\"\\n--- 3. Limpiando y Validando Datos Individuales ---\")\n",
        "\n",
        "# --- 3.1. Aplicar limpieza a las columnas de conteo ---\n",
        "df_nacimientos['Numero_Embarazos_Madre'] = limpiar_columna_conteo(\n",
        "    df_nacimientos['Numero_Embarazos_Madre'], {'>=5': 5}\n",
        ")\n",
        "df_nacimientos['Hijos_Vivos_Madre'] = limpiar_columna_conteo(\n",
        "    df_nacimientos['Hijos_Vivos_Madre'], {'11 a mas': 11}\n",
        ")\n",
        "df_nacimientos['Abortos_Madre'] = limpiar_columna_conteo(\n",
        "    df_nacimientos['Abortos_Madre'], {'11 a más': 11} # Corregido\n",
        ")\n",
        "print(\"Columnas de conteo (embarazos, hijos, abortos) limpiadas.\")\n",
        "\n",
        "# --- 3.2. Validación de Rangos Biológicos ---\n",
        "registros_antes = len(df_nacimientos)\n",
        "df_nacimientos = df_nacimientos[\n",
        "    (df_nacimientos['Edad_Madre'] >= 12) & (df_nacimientos['Edad_Madre'] <= 55) &\n",
        "    (df_nacimientos['Peso_Nacido'] >= 500) & (df_nacimientos['Peso_Nacido'] <= 5000) &\n",
        "    (df_nacimientos['Talla_Nacido'] >= 30) & (df_nacimientos['Talla_Nacido'] <= 60)\n",
        "]\n",
        "registros_despues = len(df_nacimientos)\n",
        "print(f\"Se eliminaron {registros_antes - registros_despues} registros por rangos biológicos inválidos.\")\n",
        "\n",
        "# --- 3.3. Merge Geográfico ---\n",
        "df_nacimientos['Codigo_Ubigeo'] = pd.to_numeric(df_nacimientos['Codigo_Ubigeo'], errors='coerce')\n",
        "df_ubigeos['UBIGEO_INEI'] = pd.to_numeric(df_ubigeos['UBIGEO_INEI'], errors='coerce')\n",
        "df_completo = pd.merge(\n",
        "    df_nacimientos,\n",
        "    df_ubigeos,\n",
        "    left_on='Codigo_Ubigeo',\n",
        "    right_on='UBIGEO_INEI',\n",
        "    how='left'\n",
        ")\n",
        "# Es crucial eliminar filas que no tengan departamento para la agregación\n",
        "df_completo.dropna(subset=['DEPARTAMENTO'], inplace=True)\n",
        "print(\"Merge geográfico realizado.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. AGREGACIÓN TRIMESTRAL (Creación del Dataset para el Modelo)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 4. Agregando datos por Trimestre y Departamento ---\")\n",
        "df_completo['Trimestre'] = pd.to_datetime(df_completo['Mes_Nacimiento'], format='%m').dt.quarter\n",
        "df_completo['Año_Nacimiento'] = pd.to_numeric(df_completo['Año_Nacimiento'], errors='coerce')\n",
        "df_completo.dropna(subset=['Año_Nacimiento'], inplace=True)\n",
        "df_completo['Año_Nacimiento'] = df_completo['Año_Nacimiento'].astype(int)\n",
        "\n",
        "# Función para obtener la moda (el valor más frecuente)\n",
        "def obtener_moda(x):\n",
        "    return x.mode().get(0, np.nan)\n",
        "\n",
        "df_agregado = df_completo.groupby(['Año_Nacimiento', 'Trimestre', 'DEPARTAMENTO']).agg(\n",
        "    # Variable Objetivo (y)\n",
        "    Nacimientos_Totales=('Codigo_Ubigeo', 'count'),\n",
        "\n",
        "    # Variables Predictoras (X)\n",
        "    Edad_Madre_Promedio=('Edad_Madre', 'mean'),\n",
        "    Peso_Promedio=('Peso_Nacido', 'mean'),\n",
        "    Talla_Promedio=('Talla_Nacido', 'mean'),\n",
        "    Hijos_Vivos_Promedio=('Hijos_Vivos_Madre', 'mean'),\n",
        "    Num_Embarazos_Promedio=('Numero_Embarazos_Madre', 'mean'),\n",
        "    Abortos_Promedio=('Abortos_Madre', 'mean'),\n",
        "    Nivel_Educativo_Moda=('Nivel_Instruccion_Madre', obtener_moda),\n",
        "    Estado_Civil_Moda=('Estado_Civil_Madre', obtener_moda)\n",
        ").reset_index()\n",
        "\n",
        "print(f\"Dataset agregado creado con {len(df_agregado)} filas (Trimestre-Departamento).\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. DIVISIÓN DE DATOS (Train-Test Split) ¡ANTES DE PROCESAR!\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 5. Dividiendo datos en Entrenamiento y Prueba ---\")\n",
        "# Como dijo tu profesor: división cronológica\n",
        "# Entrenamos con datos hasta 2022, probamos con 2023 en adelante.\n",
        "\n",
        "frontera_año = 2022\n",
        "df_train = df_agregado[df_agregado['Año_Nacimiento'] <= frontera_año]\n",
        "df_test = df_agregado[df_agregado['Año_Nacimiento'] > frontera_año]\n",
        "\n",
        "print(f\"Registros de Entrenamiento (<= {frontera_año}): {len(df_train)}\")\n",
        "print(f\"Registros de Prueba (> {frontera_año}): {len(df_test)}\")\n",
        "\n",
        "# --- 5.1. Definir Features (X) y Target (y) ---\n",
        "TARGET = 'Nacimientos_Totales'\n",
        "\n",
        "# Definimos nuestras listas de columnas\n",
        "numeric_features = [\n",
        "    'Año_Nacimiento', 'Edad_Madre_Promedio', 'Peso_Promedio',\n",
        "    'Talla_Promedio', 'Hijos_Vivos_Promedio', 'Num_Embarazos_Promedio',\n",
        "    'Abortos_Promedio'\n",
        "]\n",
        "categorical_features = [\n",
        "    'Trimestre', 'DEPARTAMENTO', 'Nivel_Educativo_Moda', 'Estado_Civil_Moda'\n",
        "]\n",
        "\n",
        "X_train = df_train[numeric_features + categorical_features]\n",
        "y_train = df_train[TARGET]\n",
        "\n",
        "X_test = df_test[numeric_features + categorical_features]\n",
        "y_test = df_test[TARGET]\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. DEFINICIÓN DEL PIPELINE DE PREPROCESAMIENTO\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 6. Definiendo Pipelines de Preprocesamiento ---\")\n",
        "# Esto resuelve tu \"PELIGRO\" de qué hacer con las columnas\n",
        "# Cada grupo se trata por separado.\n",
        "\n",
        "# Pipeline para variables NUMÉRICAS\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Rellena NaNs con la mediana\n",
        "    ('scaler', MinMaxScaler()) # Normaliza los datos (escala 0-1)\n",
        "])\n",
        "\n",
        "# Pipeline para variables CATEGÓRICAS\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Rellena NaNs con la moda\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # Crea variables dummy\n",
        "])\n",
        "\n",
        "# Usamos ColumnTransformer para aplicar los pipelines a las columnas correctas\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 7. Entrenando y Evaluando Modelos ---\")\n",
        "\n",
        "# --- 7.1. Definir los modelos que pidió tu profesor ---\n",
        "modelos = {\n",
        "    \"Regresión Lineal\": LinearRegression(),\n",
        "    \"Árbol de Decisión\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1)\n",
        "}\n",
        "\n",
        "resultados = []\n",
        "\n",
        "for nombre, modelo_base in modelos.items():\n",
        "\n",
        "    # 1. Crear el Pipeline COMPLETO (Preprocesar -> Modelar)\n",
        "    pipeline_modelo = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', modelo_base)\n",
        "    ])\n",
        "\n",
        "    # 2. Entrenar el pipeline SOLO con datos de TRAIN\n",
        "    print(f\"Entrenando {nombre}...\")\n",
        "    pipeline_modelo.fit(X_train, y_train)\n",
        "\n",
        "    # 3. Evaluar el pipeline SOLO con datos de TEST\n",
        "    y_pred = pipeline_modelo.predict(X_test)\n",
        "\n",
        "    # 4. Calcular Métricas\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    resultados.append({\n",
        "        \"Modelo\": nombre,\n",
        "        \"R² (Test)\": r2,\n",
        "        \"RMSE (Test)\": rmse\n",
        "    })\n",
        "    print(f\"Resultados de {nombre}: R² = {r2:.4f}, RMSE = {rmse:.4f}\\n\")\n",
        "\n",
        "# --- 7.2. Tabla Comparativa de Resultados ---\n",
        "df_resultados = pd.DataFrame(resultados).set_index(\"Modelo\")\n",
        "print(\"\\n--- TABLA COMPARATIVA DE MÉTRICAS (R² y RMSE) ---\")\n",
        "print(df_resultados)\n",
        "print(\"\\n(R² cercano a 1 es mejor. RMSE cercano a 0 es mejor)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. ANÁLISIS DE IMPORTANCIA DE VARIABLES (Random Forest)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 8. Analizando Importancia de Variables (Random Forest) ---\")\n",
        "\n",
        "# Re-entrenamos el pipeline de Random Forest para extraer los nombres\n",
        "# (Ya estaba entrenado, pero lo hacemos explícito para más claridad)\n",
        "pipeline_rf = modelos[\"Random Forest\"] # Modelo base\n",
        "pipeline_completo_rf = Pipeline(steps=[('preprocessor', preprocessor), ('model', pipeline_rf)])\n",
        "pipeline_completo_rf.fit(X_train, y_train)\n",
        "\n",
        "# Obtener los nombres de las features después del OneHotEncoding\n",
        "try:\n",
        "    nombres_cat = pipeline_completo_rf.named_steps['preprocessor'] \\\n",
        "                 .named_transformers_['cat'] \\\n",
        "                 .named_steps['onehot'] \\\n",
        "                 .get_feature_names_out(categorical_features)\n",
        "\n",
        "    # Todos los nombres de features\n",
        "    nombres_features = numeric_features + list(nombres_cat)\n",
        "\n",
        "    # Obtener las importancias\n",
        "    importancias = pipeline_completo_rf.named_steps['model'].feature_importances_\n",
        "\n",
        "    # Crear un DataFrame de importancias\n",
        "    df_importancia = pd.DataFrame({\n",
        "        'Variable': nombres_features,\n",
        "        'Importancia': importancias\n",
        "    }).sort_values(by='Importancia', ascending=False)\n",
        "\n",
        "    print(\"\\n--- Top 15 Variables Más Influyentes ---\")\n",
        "    print(df_importancia.head(15))\n",
        "\n",
        "    # --- Gráfico de Importancia ---\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(\n",
        "        data=df_importancia.head(15),\n",
        "        x='Importancia',\n",
        "        y='Variable',\n",
        "        palette='viridis'\n",
        "    )\n",
        "    plt.title('Top 15 Variables Más Influyentes (Random Forest)')\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al obtener importancia de variables: {e}\")\n",
        "    print(\"Asegúrate de que el modelo esté entrenado.\")\n",
        "\n",
        "print(\"\\n--- ANÁLISIS COMPLETADO ---\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1tgFh7_fxHRw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}